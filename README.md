# Decoding_Regression
PyTorch-based implementation of the decoding-based regression framework introduced by Song and Bahri (Google DeepMind, 2025).
Decoding_Regression is a comprehensive PyTorch-based implementation of the decoding-based regression framework introduced by Song and Bahri (Google DeepMind, 2025). This repository encapsulates a novel approach to regression tasks where numeric outputs are tokenized into discrete sequences and modeled autoregressively using transformer decoders, contrasting with traditional pointwise and histogram-based regression heads. The implementation rigorously adheres to the paper's specifications, including tokenization schemes, model architectures, theoretical underpinnings, and empirical validations across synthetic and real-world datasets. It targets researchers and practitioners in machine learning seeking to explore flexible, distribution-agnostic regression methods leveraging language model-inspired techniques.

# Technical Foundation 

The core innovation lies in representing continuous numeric targets ğ‘¦âˆˆğ‘…yâˆˆR as sequences of discrete tokens (ğ‘¡1,ğ‘¡2,â€¦,ğ‘¡ğ¾)âˆˆğ‘‰ğ¾(t 1â€‹ ,t 2 ,â€¦,t Kâ€‹ )âˆˆV K , where ğ‘‰V is afinite vocabulary (e.g., {0,1â€¦,ğµâˆ’1}{0,1,â€¦,Bâˆ’1} for base-ğµB). This enables autoregressive modeling via ğ‘ğœƒ(ğ‘¦âˆ£ğ‘¥)=âˆğ‘˜=1ğ¾ğ‘ğœƒ(ğ‘¡ğ‘˜âˆ£ğœ™(ğ‘¥),ğ‘¡1,â€¦,ğ‘¡ğ‘˜âˆ’1)p Î¸â€‹ (yâˆ£x)=âˆ k=1Kâ€‹ p Î¸â€‹ (t kâ€‹ âˆ£Ï•(x),t 1 ,â€¦,t kâˆ’1 ), where ğœ™(ğ‘¥) Ï•(x) is a feature representation from an encoder, typically a multi-layer perceptron (MLP). The approach contrasts with conventional regression heads:

1. Pointwise Head: Outputs a scalar ğ‘¦^=ğ‘“(ğœ™(ğ‘¥))yâ€‹ =f(Ï•(x)) optimized via mean squared error (MSE), â„“=(ğ‘¦âˆ’ğ‘¦^)2â„“=(yâˆ’ ^â€‹ ) 2 , assuming a unimodal distribution.

2. Histogram (Riemann) Head: Models ğ‘(ğ‘¦âˆ£ğ‘¥)p(yâˆ£x) as a piecewise-constant distribution over ğ‘›n bins, parameterized by softmax over ğœ™(ğ‘¥)ğ‘‡ğ‘ŠÏ•(x) T W, with ğ‘(ğ‘¦ğ‘–âˆ£ğ‘¥)=Softmax(ğ‘–)(ğœ™(ğ‘¥)ğ‘‡ğ‘Š)p(y i âˆ£x)=Softmax (i) (Ï•(x)  W).

3. Decoding-based Head: Employs a transformer decoder to predict token sequences, trained with cross-entropy loss, ğ»(ğ‘¦,ğ‘ğœƒ)=âˆ’âˆ‘ğ‘˜=1âˆ‘ğ‘¡ğ‘˜âˆˆğ‘‰ğ¼(ğ‘¡^ğ‘˜=ğ‘¡ğ‘˜)logğ‘ğœƒ(ğ‘¡^ğ‘˜âˆ£ğ‘¡<ğ‘˜,ğœ™(ğ‘¥))H(y,p â€‹
 )=âˆ’âˆ‘ k=1Kâ€‹ âˆ‘ t kâ€‹ âˆˆVâ€‹ I( t^  kâ€‹ =t kâ€‹ )logp Î¸â€‹ ( t^  k âˆ£t <,Ï•(x)), enabling flexible density estimation over ğ‘…R.


The paperâ€™s theoretical contribution (Theorem 1) is validated empirically, asserting that for a 
ğ¾K-bit universal model, the risk ğ‘…(ğ‘“,ğ‘“^ğ‘ğ‘˜)=ğ¸âˆ«01(ğ‘“(ğ‘¦)âˆ’ğ‘“^ğ‘ğ‘˜(ğ‘¦))2ğ‘‘ğ‘¦R(f, f^â€‹  Nkâ€‹ )=Eâˆ« 01â€‹ (f(y)âˆ’ f^â€‹  Nkâ€‹ (y)) 2 dy scales as 2âˆ’2ğ‘˜12âˆ«01ğ‘“â€²(ğ‘¦)2ğ‘‘ğ‘¦+2ğ‘˜ğ‘122 âˆ’2k â€‹ âˆ« 01â€‹ f â€² (y) 2dy+ N2k, balancing bias and variance trade-offs as ğ¾K (sequence length) and ğ‘N (sample size) vary.


# Tokenization Schemes
The repository implements three tokenization strategies:

Normalized Tokenization: Maps ğ‘¦âˆˆ[yâ¡,ğ‘¦maxâ¡]yâˆˆ[y minâ€‹ ,y maxâ€‹ ] to [0,1][0,1] via ğ‘¦â€²=(y-ymin)/ğ‘¦maxâ¡âˆ’ğ‘¦min)y â€²=(yâˆ’y minâ€‹ )/(y maxâ€‹ âˆ’y minâ€‹ ), then represents ğ‘¦â€²y â€²  as abaseğµB expansion up to length ğ¾K. For example, ğ‘¦â€²=0.123y â€² =0.123 with ğµ=10,ğ¾=4B=10,K=4 yields âŸ¨1âŸ©âŸ¨2âŸ©âŸ¨3âŸ©âŸ¨0âŸ©âŸ¨1âŸ©âŸ¨2âŸ©âŸ¨3âŸ©âŸ¨0âŸ©. Detokenization reconstructs ğ‘¦=âˆ‘ğ‘–=1ğ¾ğ‘¡ğ‘–ğµâˆ’ğ‘–â‹…(ğ‘¦maxâ¡âˆ’ğ‘¦minâ¡)+ğ‘¦minâ¡y=âˆ‘ i=1Kâ€‹ t i B âˆ’i â‹…(y max âˆ’y min)+y minâ€‹ .

Unnormalized Tokenization: Generalizes IEEE-754 floating-point representation to base-ğµB, encoding ğ‘¦=ğ‘ â‹…ğµğ‘’â‹…ğ‘šy=sâ‹…B e â‹…m where ğ‘ âˆˆ{âˆ’1,+1}sâˆˆ{âˆ’1,+1}, ğ‘’âˆˆğ‘eâˆˆZ, and ğ‘šâˆˆ[0,ğµ)mâˆˆ[0,B). Tokens are âŸ¨ğ‘ âŸ©âŸ¨ğ‘ ğ‘’âŸ©âŸ¨ğ‘’1âŸ©â€¦âŸ¨ğ‘’ğ¸âŸ©âŸ¨ğ‘š1âŸ©â€¦âŸ¨ğ‘šğ‘€âŸ©âŸ¨sâŸ©âŸ¨s e âŸ©âŸ¨e 1 âŸ©â€¦âŸ¨e E âŸ©âŸ¨m 1 âŸ©â€¦âŸ¨m M âŸ©, with ğ¸E and ğ‘€M as exponent and mantissa lengths (e.g., 10âˆ’222â‹…1.23410 âˆ’222â‹…1.234 as âŸ¨+âŸ©âŸ¨âˆ’âŸ©âŸ¨2âŸ©âŸ¨2âŸ©âŸ¨2âŸ©âŸ¨1âŸ©âŸ¨2âŸ©âŸ¨3âŸ©âŸ¨4âŸ©âŸ¨+âŸ©âŸ¨âˆ’âŸ©âŸ¨2âŸ©âŸ¨2âŸ©âŸ¨2âŸ©âŸ¨1âŸ©âŸ¨2âŸ©âŸ¨3âŸ©âŸ¨4âŸ© for ğµ=10,ğ¸=3,ğ‘€=4B=10,E=3,M=4).

Hamming Distance-based Tokenization (Appendix A.3): Encodes ğ‘¦âˆˆ[0,1]yâˆˆ[0,1] into binary sequences with bounded distortion under bitwise edits, e.g., {0,1,â€¦,7}/7{0,1,â€¦,7}/7 mapped to {(000),(001),(010),(100),(111),(101),(011),(110)} {(000),(001),(010),(100),(111),(101),(011),(110)}, though less performant in practice.


# Model Architectures

1. Encoder: An MLP with configurable layers (ğ¿âˆˆ[2,5]Lâˆˆ[2,5]) and hidden units (ğ»âˆˆ[256,2048]Hâˆˆ[256,2048]), applying ReLU activations, transforming input ğ‘¥âˆˆğ‘…ğ·xâˆˆR Dğœ™
(ğ‘¥)âˆˆğ‘…ğ»Ï•(x)âˆˆR H .Pointwise Head: Linear projection ğœ™(ğ‘¥)â†’ğ‘…Ï•(x)â†’R with sigmoid activation to enforce [0,1][0,1] outputs.Histogram Head: Projects ğœ™(ğ‘¥)â†’ğ‘…ğ‘›Ï•(x)â†’R n ( ğ‘›âˆˆ[16,16384]nâˆˆ[16,16384] bins), computes softmax probabilities, and returns the expected value over bin centers.

2.Decoder Head: A transformer decoder with ğ¿âˆˆ[1,5]Lâˆˆ[1,5] layers, ğ»âˆˆ[32,256]Hâˆˆ[32,256] hidden units, and ğ‘â„âˆˆ[1,8]N hâ€‹ âˆˆ[1,8] attention heads, predicting token sequences autoregressively. Supports constrained decoding for valid numeric representations.

3. Mixture Density Network (MDN) Head: Outputs mixture weights ğœ‹âˆˆÎ”ğ‘€Ï€âˆˆÎ” M , means ğœ‡âˆˆğ‘…ğ‘€Î¼âˆˆR M , and variances ğœ2âˆˆğ‘…+ğ‘€Ïƒ 2 âˆˆR +Mâ€‹  (via ELU+1 activation), modeling ğ‘(ğ‘¦âˆ£ğ‘¥)=âˆ‘ğ‘š=1ğ‘€ğœ‹ğ‘šğ‘(ğ‘¦;ğœ‡ğ‘š,ğœğ‘š2)p(yâˆ£x)=âˆ‘ m=1Mâ€‹ Ï€ mâ€‹ N(y;Î¼ m ,Ïƒ m2 ).
  

 Training and Optimization
 
 1. Loss Functions: MSE for pointwise/histogram, cross-entropy for decoder, and negative log-likelihood for MDN.

 2. Optimizer: Adam with learning rates ğœ‚âˆˆ[10âˆ’4,5â‹…10âˆ’4]Î·âˆˆ[10 âˆ’4 ,5â‹…10 âˆ’4 ], weight decay ğœ†âˆˆ[0,1]Î»âˆˆ[0,1].Early Stopping: Patience of 5 epochs on validation loss (10% 
   split).

3. Normalization: ğ‘¥x-coordinates scaled via (ğ‘¥âˆ’ğœ‡ğ‘¥)/ğœğ‘¥(xâˆ’Î¼ xâ€‹ )/Ïƒ xâ€‹ ; ğ‘¦y-values via min-max scaling to [0,1][0,1] or shifted to [âˆ’0.5,0.5][âˆ’0.5,0.5] for 
  pointwise/MDN heads.

# Experiments

The repository replicates all experiments from Sections 4.1â€“4.5 and Appendices A.1â€“A.5:

Synthetic Curve Fitting (Section 4.1):
1. Functions: ğ‘¦=sinâ¡(ğ‘¥)y=sin(x), ğ‘¦=ğ‘’âˆ’ğ‘¥2y=e âˆ’x 2  , ğ‘¦=ğ‘¥2y=x 2  with Gaussian noise ğ‘(0,0.1)N(0,0.1).
   Evaluates MSE and visual fit, showcasing decoderâ€™s ability to handle high Lipschitz constants and wide ranges.


Real-World Regression (Section 4.2):
1, Datasets: UCI (e.g., Airfoil, Housing) and OpenML-CTR23/AMLB subsets.
2. Metrics: MSE and Kendall-Tau scores, comparing data efficiency across regimes (ğ‘âˆˆ[10,04]Nâˆˆ[10,10 4 ]).


Density Estimation (Section 4.3):

Visualizes ğ‘(ğ‘¦âˆ£ğ‘¥) p(yâˆ£x) fits (e.g., bimodal distributions) using vanilla temperature sampling (ğ‘‡â‰ˆ1.0Tâ‰ˆ1.0).
Computes negative log-likelihood (NLL) on UCI datasets, benchmarking against MDN and Riemann heads.

Ablation: Decoder Size (Section 4.4):Sweeps ğ¿âˆˆ[1,5]Lâˆˆ[1,5],ğ‘â„âˆˆ[1,8]N hâ€‹ âˆˆ[1,8], ğ»âˆˆ[32,256]Hâˆˆ[32,256], assessing overfitting and NLL trade-offs.

Ablation: Error Correction (Section 4.5):Implements repetition-based error correction (e.g., repeat ğ‘Ÿâˆˆ[1,5râˆˆ[1,5] times, majority voting), reducing outlier sensitivity in unnormalized decoding.

# Implementation Details
Dependencies: PyTorch 2.0+, NumPy, Matplotlib, Scikit-learn, Pandas, Requests.
Structure: Modular design with src/ for core logic, experiments/ for scripts, and data/ for datasets.
Hyperparameters: Fully configurable per Appendix D, e.g., ğµâˆˆ[2,10]Bâˆˆ[2,10], ğ¾âˆˆ[4,8]Kâˆˆ[4,8], ğ¸âˆˆ[1,4]
Eâˆˆ[1,4], ğ‘€âˆˆ[2,8]Mâˆˆ[2,8], ğ‘€MDNâˆˆ[1,1000]M MDNâˆˆ[1,1000].

Outputs: Results saved in results/ with plots and logs for analysis.

# Install dependencies
pip install -r requirements.txt

# Download UCI datasets
python data/uci_datasets.py

# Run experiments
python experiments/synthetic.py
python experiments/real_world.py
python experiments/density_est.py
python experiments/ablation_size.py
python experiments/ablation_error.py

Validation and Future Work
The implementation validates the paperâ€™s claims of competitiveness (e.g., decoder MSE often below pointwise/histogram) and density estimation flexibility (NLL < 0.7 on UCI tasks). Future extensions could explore alternative tokenizations (e.g., Stern-Brocot trees), multi-target regression, or convolutional encoders for vision tasks, as suggested in Section 5.

This repository serves as a technical cornerstone for advancing autoregressive regression research, bridging language modeling and numeric prediction with a robust, reproducible framework.
